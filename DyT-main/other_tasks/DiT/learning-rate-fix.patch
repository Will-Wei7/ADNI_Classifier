From a78277316a4c58e0e40e5506dbacdc346090597e Mon Sep 17 00:00:00 2001
From: Jiachen Zhu <jiachen.zhu@nyu.edu>
Date: Mon, 17 Mar 2025 19:33:17 +0000
Subject: [PATCH] learning-rate-fix

---
 train.py | 3 ++-
 1 file changed, 2 insertions(+), 1 deletion(-)

diff --git a/train.py b/train.py
index 7cfee80..3bc8c87 100644
--- a/train.py
+++ b/train.py
@@ -152,7 +152,7 @@ def main(args):
     logger.info(f"DiT Parameters: {sum(p.numel() for p in model.parameters()):,}")
 
     # Setup optimizer (we used default Adam betas=(0.9, 0.999) and a constant learning rate of 1e-4 in our paper):
-    opt = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0)
+    opt = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0)
 
     # Setup data:
     transform = transforms.Compose([
@@ -265,5 +265,6 @@ if __name__ == "__main__":
     parser.add_argument("--num-workers", type=int, default=4)
     parser.add_argument("--log-every", type=int, default=100)
     parser.add_argument("--ckpt-every", type=int, default=50_000)
+    parser.add_argument("--lr", type=float, default=4e-4)
     args = parser.parse_args()
     main(args)
-- 
2.34.1

