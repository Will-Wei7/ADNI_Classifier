From e65952277190e602649e687317a3f1df15974992 Mon Sep 17 00:00:00 2001
From: Jiachen Zhu <jiachen.zhu@nyu.edu>
Date: Mon, 17 Mar 2025 21:59:40 +0000
Subject: [PATCH] dynamic-tanh

---
 fairseq/models/wav2vec/wav2vec2.py | 29 ++++++++++++++++++++++++-----
 1 file changed, 24 insertions(+), 5 deletions(-)

diff --git a/fairseq/models/wav2vec/wav2vec2.py b/fairseq/models/wav2vec/wav2vec2.py
index 0faba77f..7d880f54 100644
--- a/fairseq/models/wav2vec/wav2vec2.py
+++ b/fairseq/models/wav2vec/wav2vec2.py
@@ -305,6 +305,25 @@ class Wav2Vec2Config(FairseqDataclass):
     )
 
 
+class DynamicTanh(nn.Module):
+    def __init__(self, normalized_shape, alpha_init_value=0.5):
+        super().__init__()
+        self.normalized_shape = normalized_shape
+        self.alpha_init_value = alpha_init_value
+
+        self.alpha = nn.Parameter(torch.ones(1) * alpha_init_value)
+        self.weight = nn.Parameter(torch.ones(normalized_shape))
+        self.bias = nn.Parameter(torch.zeros(normalized_shape))
+
+    def forward(self, x):
+        x = torch.tanh(self.alpha * x)
+        x = x * self.weight + self.bias
+        return x
+
+    def extra_repr(self):
+        return f"normalized_shape={self.normalized_shape}, alpha_init_value={self.alpha_init_value}"
+
+
 @register_model("wav2vec2", dataclass=Wav2Vec2Config)
 class Wav2Vec2Model(BaseFairseqModel):
     def __init__(self, cfg: Wav2Vec2Config):
@@ -617,7 +636,7 @@ class Wav2Vec2Model(BaseFairseqModel):
         features_pen = features.float().pow(2).mean()
 
         features = features.transpose(1, 2)
-        features = self.layer_norm(features)
+        features = self.dynamic_tanh(features)
         unmasked_features = features.clone()
 
         if padding_mask is not None and padding_mask.any():
@@ -1066,7 +1085,7 @@ class TransformerEncoder(nn.Module):
             [self.build_encoder_layer(args, layer_idx=ii) for ii in range(encoder_layers)]
         )
         self.layer_norm_first = args.layer_norm_first
-        self.layer_norm = LayerNorm(self.embedding_dim)
+        self.layer_norm = DynamicTanh(self.embedding_dim)
         self.layerdrop = args.encoder_layerdrop
 
         self.apply(init_bert_params)
@@ -1217,7 +1236,7 @@ class ConformerEncoder(TransformerEncoder):
             [self.build_encoder_layer(args) for _ in range(args.encoder_layers)]
         )
         self.layer_norm_first = args.layer_norm_first
-        self.layer_norm = LayerNorm(self.embedding_dim)
+        self.layer_norm = DynamicTanh(self.embedding_dim)
         self.layerdrop = args.encoder_layerdrop
 
         self.apply(init_bert_params)
@@ -1305,12 +1324,12 @@ class TransformerSentenceEncoderLayer(nn.Module):
         self.layer_norm_first = layer_norm_first
 
         # layer norm associated with the self attention layer
-        self.self_attn_layer_norm = LayerNorm(self.embedding_dim)
+        self.self_attn_layer_norm = DynamicTanh(self.embedding_dim)
         self.fc1 = nn.Linear(self.embedding_dim, ffn_embedding_dim)
         self.fc2 = nn.Linear(ffn_embedding_dim, self.embedding_dim)
 
         # layer norm associated with the position wise feed-forward NN
-        self.final_layer_norm = LayerNorm(self.embedding_dim)
+        self.final_layer_norm = DynamicTanh(self.embedding_dim)
 
     def forward(
         self,
-- 
2.34.1

